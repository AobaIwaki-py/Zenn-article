---
title: "CVAEをPyTorchで実装してみた"
emoji: "🐥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["PyTorch","CVAE","Python"]
published: false
---

# 参考記事一覧
- [PyTorchでCVAEを実装してみた - Qiita](https://qiita.com/isuya/items/a856971647ba2390f5fe)
- [Variational Autoencoder徹底解説 - Qiita](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24#1-variational-autoencoder%E3%81%A8%E3%81%AF)
- [最尤法によるパラメータ推定の基礎を理解する(二項分布と正規分布のパラメータの最尤推定量の導出) - Qiita](https://qiita.com/g-k/items/4b2d557c24558074f07f)
- [イェンセン(Jensen)の不等式の直感的理解 - Qiita](https://qiita.com/kenmatsu4/items/26d098a4048f84bf85fb)
- [Jensen's inequality (イェンセンの不等式)のシンプルな証明](https://mcm-www.jwu.ac.jp/~konno/pdf/statg-1-8r.pdf)
- [【基礎から理解しよう】KLダイバージェンスの意味 - DXコンサルの日進月歩奮闘記](https://dx-consultant-fast-evolving.com/the-meaning-of-kl-divergence/)
- [モデル選択：周辺尤度最大化 | Pyroで実践するベイズ機械学習](https://pyro-book.data-hacker.net/docs/model_selection_01/)
- [機械学習のための確率基礎とベイズの定理 - HELLO CYBERNETICS](https://www.hellocybernetics.tech/entry/2016/10/05/163856#確率の乗法定理)
- [変分法をごまかさずに変分ベイズの説明をする - StatModeling Memorandum](https://statmodeling.hatenablog.com/entry/variational-bayesian-inference-1)
- [イェンセンの不等式とKLダイバージェンスの非負性｜Python実装で理解する変分推論(VariationalInference) #2 - Liberal Art’s diary](https://lib-arts.hatenablog.com/entry/VariationalInference2)
- [上に凸，下に凸な関数と二階微分 | 高校数学の美しい物語](https://manabitimes.jp/math/927)
- [1.1.8-10：カルバック・ライブラー・ダイバージェンスとイェンゼンの不等式【『トピックモデル』の勉強ノート】 - からっぽのしょこ](https://www.anarchive-beta.com/entry/2019/05/03/005902#1110-イェンゼンの不等式)
- [カルバックライブラーダイバージェンスとその周辺](https://zenn.dev/akio_tomiya/articles/29a3bb2d2c23f3)
- [正規分布間のKLダイバージェンスの導出 - 唯物是真 @Scaled\_Wurm](https://sucrose.hatenablog.com/entry/2013/07/20/190146)
- [正規分布間のKLダイバージェンス(KL-divergence)の値をグラフ化して把握する - あつまれ統計の森](https://www.hello-statisticians.com/explain-terms-cat/kl_divergence1.html)

# この記事の概要
全体を通して下の記事を大変参考にさせていただきました。
- [Variational Autoencoder徹底解説 - Qiita](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24#1-variational-autoencoderとは)

この記事では、VAEの理論的な内容について詳しくまとめています。全体の流れは上の記事と同様ですが、焼き直しにならないために自分なりにわかりやすくまとめました。特に、対数尤度から変分下限への式変形で自分は詰まったので、言葉を尽くしてもれなく説明したつもりです。その過程で参考にした記事も適宜紹介しています。

# VAEについて
## VAEの要点
AEは、入力をEncoderに通して潜在変数$z$を出力します。
1. VAEは、この潜在変数$z$の分布に標準正規分布$z\sim N(0,1)$を仮定します。
2. 仮定より、$z$が正規分布に従います。また、学習時には正規分布に従う乱数を取り入れています。この乱数のブレによって似た形状のものを近くに寄せる効果があります。従って、学習済みのEnocderから出力される$z$の分布は、同じ入力に対して比較的近いところに分布します。
## 理論的な概要
VAEの生成モデルの目的は、データの分布である$p(X)$を推定することです。画像を対象と考えると、非常に高次元なデータですが実際にデータが存在する部分は非常に限られています。そこで、それを低次元の潜在変数で表現することを考えます。つまり、Encoderとは、高次元の入力$X$と低次元の潜在変数$z$の対応関係を学習して出力するものだといえます。

VAEは、この潜在変数が正規分布となるように学習し、$p(X)$を推定します。今、この確率分布を表現するパラメータがあると仮定します。すると、$p(X)$に関する最尤法を解くことで、$X$をよく表現する$p(X)$のパラメータを得ることができます。

VAEでは、そのパラメータを求めるためにニューラルネットワークを利用します。従って、求めるパラメータはEncoder及びDecoderが持つパラメータ$\phi,\theta$です。入力$X$、潜在変数$z$、パラメータ$\phi,\theta$の関係をグラフィカルモデルとして以下に示します。

![](https://storage.googleapis.com/zenn-user-upload/d29d723bb3c4-20230205.png)

### データにフィットしたパラメータを求める手順
#### 忙しい人向け
1. 対数尤度$\log p(X)$を変分下限$L(X,z)$で下から抑えて変分下限の最大化を行います。
$$\log p(X)\geq L(X,z)$$
2. 計算すると変分下限は、次のように表せます。$D_{KL}$は、KLダイバージェンス、$E[\log p(X)]$は、対数尤度の期待値です。
$$L(X,z)=-D_{KL}+E[\log p(X)]$$
3. 従って、変分下限を最大化することはKLダイバージェンスを最小化することに等しいです。そこで、$-D_{KL}$を計算します。
4. 目標は、変分下限の最大化なので変分下限にマイナスをかけたものを損失関数とします。$E[\log p(X)]$のよさは、BCELoss(交差エントロピー損失)で評価されるので、それも損失関数に加えます。すると、次のようになります。
$$Loss\_func=-(-D_{KL})+BCELoss$$
#### 最尤法について
最尤法について学ぶ際に下の記事を特に参考にしました。最尤法の理解はまだまだだと感じていますので、不十分な点があればご指摘いただけると幸いです。
- [最尤法によるパラメータ推定の基礎を理解する(二項分布と正規分布のパラメータの最尤推定量の導出) - Qiita](https://qiita.com/g-k/items/4b2d557c24558074f07f)
#### 対数尤度の最大化問題を変分下限の最大化問題に置き換える
最尤法を用いて$p(X)$の尤度を最大にするパラメータ$\phi,\theta$を求めることを考えます。しかし、$p(X)$の尤度を最大化するのは一般に困難です。そこで、$p(X)$の対数をとった対数尤度$\log p(X)$について考えます。$p(X)$と$log(P(X))$は単調な関係であるため、$p(X)$を最大化する問題と$\log p(X)$を最大化する問題は同値になります。どうして対数を取ると計算が楽になるのかというと、下記に示すJensen's inequality (イェンセンの不等式)を用いることができるからです。Jensen's inequality については、以下が参考になります。シンプルな証明が知りたい方は[こちら](https://mcm-www.jwu.ac.jp/~konno/pdf/statg-1-8r.pdf)をご覧ください。但し、こちらで証明されているのは関数$f(x)$(証明内では、$h(x)$)が下に凸な場合です。今回は、関数$f(x)$が上に凸であることを利用するので不等式の向きが逆になります。自分はここで混乱したので気をつけてください。

- [イェンセン(Jensen)の不等式の直感的理解 - Qiita](https://qiita.com/kenmatsu4/items/26d098a4048f84bf85fb)

上に凸な関数$f(x)$についてJensen's inequalityより、以下の式が成り立ちます。

$$f\left(\int p(x)g(x)dx\right)\geq\int p(x)f(g(x))dx$$

$\log$は上に凸な関数なので次が成り立ちます。$\log$が上に凸であるということに疑問を感じる方は[こちら](https://manabitimes.jp/math/927)をご覧ください。

$$\log\int p(x)g(x)dx\geq\int p(x)\log g(x)dx$$

この式を用いて、対数尤度の最大化問題をさらに変分下限の最大化問題に置き換えます。下の式変形を説明します。
1. まず、$\log p(X)$の$z$に関する周辺尤度の式を立てます。周辺尤度がよくわからない方は、[こちら](https://pyro-book.data-hacker.net/docs/model_selection_01/)をご覧ください。
2. 次に、$q(z \mid X)/q(z \mid X)=1$を掛けます。
3. Jensen's inequality によって対数尤度の下限を見積もります。
4. その値を変分下限$L(X, z)$とおきます。

$$
\begin{aligned}
\log p(X) & =\log \int p(X, z) d z \\
& =\log \int q(z \mid X) \frac{p(X, z)}{q(z \mid X)} d z \\
& \geq \int q(z \mid X) \log \frac{p(X, z)}{q(z \mid X)} d z \\
& =L(X, z)
\end{aligned}
$$

$L(X,z)$は変分下限(ELBO:Evidence Lower BOund)と呼ばれるものです。
上に示した大小関係から対数尤度と変分下限の間には下図のようなギャップが存在します。
![](https://storage.googleapis.com/zenn-user-upload/75ed11d78397-20230206.png)

このギャップを計算するために、対数尤度と変分下限の差をとってみます。式変形について説明します。
1. 対数尤度と変分下限の差をとります。
2. $L(X,z)$を積分の形に展開します。
3. 第1項に$\int q(z\mid X)dz$を掛けます。$q(z\mid X)$は、$z$に関する確率であるため積分すると1になります。従って、計算に影響を与えません。
4. 第1項において、$\log p(X)$は、$z$を変数に持たないため、定数として扱えます。従って、$\int$の中に入れることができます。第2項において、乗法定理より、$p(z, X)=p(z\mid X)p(X)$と変換します。
5. 第２項において、対数の性質から積を和の形に変換します。
6. $\int q(z\mid X)$を共通因数として括ります。この時、$\log p(X)$が打ち消し合います。
7. 対数の性質を用いて和を積の形に変換します。
8. これは、Kullback Leiblier Divergence(カルバックライブラーダイバージェンス)の定義そのものであるため$D_{KL}$とおきます。最後の値が0以上となるのは、Kullback Leiblier Divergenceが常に0以上であるためです。

$$
\begin{aligned}
&\log p(X)-L(X,z)\\
&=\log p(X)-\int q(z\mid X)\log\frac{p(X,z)}{q(z\mid X)}dz\\
&=\log p(X)\int q(z\mid X)dz-\int q(z\mid X)\log\frac{p(X,z)}{q(z\mid X)}dz\\
&=\int q(z\mid X)\log p(X)dz-\int q(z\mid X)\log\frac{p(z\mid X)p(X)}{q(z\mid X)}dz\\
&=\int q(z\mid X)\log p(X)dz-\int q(z\mid X)[\log p(z\mid X)+\log p(X)-\log (z\mid X)]dz\\
&=\int q(z\mid X)[-\log p(z\mid X)+\log q(z\mid X)]dz\\
&=\int q(z\mid X)\log\frac{q(z\mid X)}{p(z\mid X)}dz\\
&=D_{KL}[q(z\mid X)||p(z\mid X)|]\geq 0
\end{aligned}
$$

#### Kullback Leiblier Divergenceとは
以下の記事を特に参考にしました。

- [【基礎から理解しよう】KLダイバージェンスの意味 - DXコンサルの日進月歩奮闘記](https://dx-consultant-fast-evolving.com/the-meaning-of-kl-divergence/)
- [カルバックライブラーダイバージェンスとその周辺](https://zenn.dev/akio_tomiya/articles/29a3bb2d2c23f3)
- [正規分布間のKLダイバージェンス(KL-divergence)の値をグラフ化して把握する - あつまれ統計の森](https://www.hello-statisticians.com/explain-terms-cat/kl_divergence1.html)

Kullback Leiblier Divergence(カルバックライブラーダイバージェンス)は、二つの確率分布の類似度を表す値です。以下のように定義されます。

$$D(p||q)=\int p(x)\log\frac{p(x)}{q(x)}dx$$

二つの確率分布が同一である場合、KLダイバージェンスの値は0となります。また、KLダイバージェンスが非負であることは、[こちら](https://zenn.dev/akio_tomiya/articles/29a3bb2d2c23f3)で証明されています。従って、KLダーバージェンスを小さくして0に近づけることは、二つの確率分布の形状を同一にするのと等価といえます。今回の場合、$p(X)$を多辺量正規分布に近づけることを目標としています。
ここまでの計算を図に示すと以下のようになります。
![](https://storage.googleapis.com/zenn-user-upload/8f62181ff8bd-20230206.png)
また、以上の結果を用いて変分下限を表現すると以下のようになります。2行目の式では、パラメータ$\phi,\theta$を明示的に表しています。

$$
\begin{aligned}
L(X,z)&=\log p(X)-D_{KL}[q(z\mid X)||p(z\mid X)]\\
&=\log p(X)-D_{KL}[q_{\phi}(z\mid X)||p_{\theta}(z\mid X)]
\end{aligned}
$$

#### $\mathbf{E}_{q(z \mid X)}[\log p(X \mid z)]$を最大化するには

#### 最終的な損失関数

### Reparameterization Trick
VAEの構造では、逆伝播の途中に$z\sim N(\mu(X),\sigma(X))$という確率分布が入ります。分布のままでは、逆伝播の計算ができないので乱数$\epsilon\sim N(0,1)$を用いて潜在変数$z$を確定させます。式では以下のように表せます。

$$z=\mu(X)+\epsilon\cdot\sigma(X)$$

