---
title: "VAEにおける理論を数式で追う~変分下限の導出からReparameterization Trickまで~"
emoji: "🐥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["PyTorch","Python"]
published: false
---


# この記事の概要
全体を通して下の記事を大変参考にさせていただきました。VAE概観についてはこちらを参考にしてください。

- [Variational Autoencoder徹底解説 - Qiita](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24#1-variational-autoencoderとは)


この記事では上の記事の「3. VAEの理論的な概要」を読んで自分の理解を添えて対数尤度、変分下限、KLダイバージェンスの導出について解説しています。式変形の1行1行にコメントをつけたり、数学的な背景が必要な箇所については適宜参考記事を入れるなどはしましたが、基本的には勉強ノートに近い形式となっています。

# VAEの理論的な概要
VAEの生成モデルの目的は、データの分布である$p(X)$を推定することです。画像を対象と考えると、非常に高次元なデータですが実際にデータが存在する部分は非常に限られています。そこで、それを低次元の潜在変数で表現することを考えます。つまり、Encoderとは、高次元の入力$X$と低次元の潜在変数$z$の対応関係を学習して出力するものだといえます。

VAEは、この潜在変数が正規分布となるように学習し、$p(X)$を推定します。今、この確率分布を表現するパラメータがあると仮定します。すると、$p(X)$に関する最尤法を解くことで、$X$をよく表現する$p(X)$のパラメータを得ることができます。

VAEでは、そのパラメータを求めるためにニューラルネットワークを利用します。従って、求めるパラメータはEncoder及びDecoderが持つパラメータ$\phi,\theta$です。入力$X$、潜在変数$z$、パラメータ$\phi,\theta$の関係をグラフィカルモデルとして以下に示します。

![](https://storage.googleapis.com/zenn-user-upload/d29d723bb3c4-20230205.png)

## 忙しい人向け
1. 対数尤度$\log p(X)$を変分下限$L(X,z)$で下から抑えて変分下限の最大化を行います。
$$\log p(X)\geq L(X,z)$$
2. 計算すると変分下限は、次のように表せます。$D_{KL}$は、KLダイバージェンス、$E[\log p(X)]$は、対数尤度の期待値です。
$$L(X,z)=-D_{KL}+E[\log p(X)]$$
3. 従って、変分下限を最大化することはKLダイバージェンスを最小化することに等しいです。そこで、$-D_{KL}$を計算します。KLダイバージェンスは次のように表されるものとします。
$$D_{KL}=\frac{1}{2}\left[-\log \sigma^2+\sigma^2-1+\mu^2\right]$$
$D_{KL}$は非負であるため定義を満たしているか図示して確認します。下に$y=-\log x^2+x^2-1$を示します。$D_{KL}$は、これに非負の数$\mu^2$を足すので必ず正であることが確認できました。
![](https://storage.googleapis.com/zenn-user-upload/14a6db2dcc29-20230206.png)
4. 目標は、変分下限の最大化なので変分下限にマイナスをかけたものを損失関数とします。$E[\log p(X)]$のよさは、BCELoss(交差エントロピー損失)で評価されるので、それも損失関数に加えます。すると、次のようになります。
$$Loss\_func=-(-D_{KL})+BCELoss$$
ネット上の多くの記事では、上で定義した$D_{KL}$にマイナスを掛けた形で$D_{KL}$を定義しているものが多いです。そのため、損失関数は上記のようになりません。私は、定義から$D_{KL}$と表示されるものは非負である方が自然だと考えたためこのような表記法を取りました。

## 対数尤度の最大化問題を変分下限の最大化問題に置き換える
最尤法を用いて$p(X)$の尤度を最大にするパラメータ$\phi,\theta$を求めることを考えます。しかし、$p(X)$の尤度を最大化するのは一般に困難です。そこで、$p(X)$の対数をとった対数尤度$\log p(X)$について考えます。$p(X)$と$log(P(X))$は単調な関係であるため、$p(X)$を最大化する問題と$\log p(X)$を最大化する問題は同値になります。

どうして対数を取ると計算が楽になるのかというと、下記に示すJensen's inequality (イェンセンの不等式)を用いることができるからです。Jensen's inequality については、以下が参考になります。

- [イェンセン(Jensen)の不等式の直感的理解 - Qiita](https://qiita.com/kenmatsu4/items/26d098a4048f84bf85fb)

シンプルな証明が知りたい方は[こちら](https://mcm-www.jwu.ac.jp/~konno/pdf/statg-1-8r.pdf)をご覧ください。但し、こちらで証明されているのは関数$f(x)$(証明内では、$h(x)$)が下に凸な場合です。今回は、関数$f(x)$が上に凸であることを利用するので不等式の向きが逆になります。自分はここで混乱したので気をつけてください。


上に凸な関数$f(x)$についてJensen's inequalityより、以下の式が成り立ちます。

$$f\left(\int p(x)g(x)dx\right)\geq\int p(x)f(g(x))dx$$

$\log$は上に凸な関数なので次が成り立ちます。$\log$が上に凸であるということに疑問を感じる方は[こちら](https://manabitimes.jp/math/927)をご覧ください。

$$\log\int p(x)g(x)dx\geq\int p(x)\log g(x)dx$$

この式を用いて、対数尤度の最大化問題をさらに変分下限の最大化問題に置き換えます。
### 式変形の説明

1. まず、$\log p(X)$の$z$に関する周辺尤度の式を立てます。周辺尤度がよくわからない方は、[こちら](https://pyro-book.data-hacker.net/docs/model_selection_01/)をご覧ください。
2. 次に、$q(z \mid X)/q(z \mid X)=1$を掛けます。
3. Jensen's inequality によって対数尤度の下限を見積もります。
4. その値を変分下限$L(X, z)$とおきます。

$$
\begin{aligned}
\log p(X) & =\log \int p(X, z) d z \\
& =\log \int q(z \mid X) \frac{p(X, z)}{q(z \mid X)} d z \\
& \geq \int q(z \mid X) \log \frac{p(X, z)}{q(z \mid X)} d z \\
& =L(X, z)
\end{aligned}
$$

## 対数尤度と変分下限の差を求める

$L(X,z)$は変分下限(ELBO:Evidence Lower BOund)と呼ばれるものです。
上に示した大小関係から対数尤度と変分下限の間には下図のようなギャップが存在します。
![](https://storage.googleapis.com/zenn-user-upload/75ed11d78397-20230206.png)

このギャップを計算するために、対数尤度と変分下限の差をとってみます。
### 式変形の説明

1. 対数尤度と変分下限の差をとります。
2. $L(X,z)$を積分の形に展開します。
3. 第1項に$\int q(z\mid X)dz$を掛けます。$q(z\mid X)$は、$z$に関する確率であるため積分すると1になります。従って、計算に影響を与えません。
4. 第1項において、$\log p(X)$は、$z$を変数に持たないため、定数として扱えます。従って、$\int$の中に入れることができます。第2項において、乗法定理より、$p(z, X)=p(z\mid X)p(X)$と変換します。
5. 第２項において、対数の性質から積を和の形に変換します。
6. $\int q(z\mid X)$を共通因数として括ります。この時、$\log p(X)$が打ち消し合います。
7. 対数の性質を用いて和を積の形に変換します。
8. これは、Kullback Leiblier Divergence(カルバックライブラーダイバージェンス)の定義そのものであるため$D_{KL}$とおきます。最後の値が0以上となるのは、Kullback Leiblier Divergenceが常に0以上であるためです。

$$
\begin{aligned}
&\log p(X)-L(X,z)\\
&=\log p(X)-\int q(z\mid X)\log\frac{p(X,z)}{q(z\mid X)}dz\\
&=\log p(X)\int q(z\mid X)dz-\int q(z\mid X)\log\frac{p(X,z)}{q(z\mid X)}dz\\
&=\int q(z\mid X)\log p(X)dz-\int q(z\mid X)\log\frac{p(z\mid X)p(X)}{q(z\mid X)}dz\\
&=\int q(z\mid X)\log p(X)dz-\int q(z\mid X)[\log p(z\mid X)+\log p(X)-\log (z\mid X)]dz\\
&=\int q(z\mid X)[-\log p(z\mid X)+\log q(z\mid X)]dz\\
&=\int q(z\mid X)\log\frac{q(z\mid X)}{p(z\mid X)}dz\\
&=D_{KL}[q(z\mid X)||p(z\mid X)|]\geq 0
\end{aligned}
$$

## Kullback Leiblier Divergenceとは
以下の記事を特に参考にしました。

- [【基礎から理解しよう】KLダイバージェンスの意味 - DXコンサルの日進月歩奮闘記](https://dx-consultant-fast-evolving.com/the-meaning-of-kl-divergence/)
- [カルバックライブラーダイバージェンスとその周辺](https://zenn.dev/akio_tomiya/articles/29a3bb2d2c23f3)
- [正規分布間のKLダイバージェンス(KL-divergence)の値をグラフ化して把握する - あつまれ統計の森](https://www.hello-statisticians.com/explain-terms-cat/kl_divergence1.html)

Kullback Leiblier Divergence(カルバックライブラーダイバージェンス)は、二つの確率分布の類似度を表す値です。KLダイバージェンスは、交差エントロピーとシャノンエントロピーの差として定義されます。

$$
\begin{aligned}
D_{KL}(p||q)&=-\int p(x)\log_2q(x)dx-\left[-\int p(x)\log_2(x)dx\right]\\
&=\int p(x)\log\frac{p(x)}{q(x)}dx
\end{aligned}$$

二つの確率分布が同一である場合、KLダイバージェンスの値は0となります。また、KLダイバージェンスが非負であることは、[こちら](https://zenn.dev/akio_tomiya/articles/29a3bb2d2c23f3)で証明されています。従って、KLダーバージェンスを小さくして0に近づけることは、二つの確率分布の形状を同一にするのと等価といえます。今回の場合、$p(X)$を多変量正規分布に近づけることを目標としています。
ここまでの計算を図に示すと以下のようになります。
![](https://storage.googleapis.com/zenn-user-upload/8f62181ff8bd-20230206.png)
また、以上の結果を用いて変分下限を表現すると以下のようになります。2行目の式では、パラメータ$\phi,\theta$を明示的に表しています。

$$
\begin{aligned}
L(X,z)&=\log p(X)-D_{KL}[q(z\mid X)||p(z\mid X)]\\
&=\log p(X)-D_{KL}[q_{\phi}(z\mid X)||p_{\theta}(z\mid X)]
\end{aligned}
$$

ここで、KLダイバージェンスは下記のように3つに分解できます。

### 式変形の説明

1. KLダイバージェンスは、交差エントロピーと自己エントロピーの差であると説明しました。従って、$q_{\phi}$を確率変数とした期待値の形式で表現することができます。ここがよくわからない方は、前述の$D_{KL}$の定義をご確認ください。その際、下式とは$p,q$の順序が異なるので混乱しないように注意してください。
2. ベイズの定理より、次が成り立ちます。ベイズの定理がよくわからない方は、[こちら](https://manabitimes.jp/math/804)をご覧ください。
$$p_{\theta}(z\mid X)=\frac{p_{\theta}(X\mid z)p(z)}{p(X)}$$
この式と対数の性質を使って和の形に変形しています。
3. $\log p(X)$は、$q_{\phi}(z\mid X)$に依存しないので外に出すことができます。
4. 最終的に$D_{KL}$の形を作るために期待値の計算を二つに分けます。
5. 第1項が$D_{KL}$の定義そのものであるため、KLダイバージェンスとして表示します。

$$
\begin{aligned}
&D_{KL}[q_{\phi}(z\mid X)||p_{\theta}(z\mid X)]\\
&=E_{q_{\phi}(z\mid X)}[\log q_{\phi}(z\mid X)-\log p_{\theta}(z\mid X)]\\
&=E_{q_{\phi}(z\mid X)}[\log q_{\phi}(z\mid X)-\log p_{\theta}(X\mid z)-\log p(z)+\log p(X)]\\
&=E_{q_{\phi}(z\mid X)}[\log q_{\phi}(z\mid X)-\log p_{\theta}(X\mid z)-\log p(z)]+\log p(X)\\
&=E_{q_{\phi}(z\mid X)}[\log q_{\phi}(z\mid X)-\log p(z)]-E_{q_{\phi}(z\mid X)}[\log p_{\theta}(X\mid z)]+\log p(X)\\
&=D_{KL}[q_{\phi}(z\mid X)||p(z)]-E_{q_{\phi}(z\mid X)}[\log p_{\theta}(X\mid z)]+\log p(X)
\end{aligned}
$$

## 変分下限を対数尤度を用いずに表す

以上の結果を踏まえて変分下限を計算すると以下のようになります。

$$L(X,z)=-D_{KL}[q(z\mid X)||p(z)]+E_{q_{\phi}(z\mid X)}[\log p_{\theta}(X\mid z)]$$

元々最大化する対象であった対数尤度を上手く消去することができました。ここで、$E_{q_{\phi}(z\mid X)}[\log p_{\theta}(X\mid z)]$は、Encoder,Decoderを表しています。つまり、確率$q_{\phi}(z\mid X)$の下で対数尤度$\log p_{\theta}(X\mid z)$を最大化したいということを表しています。
これを最大化するには、Binary Cross Entropy Loss(BCELoss)を計算してそれを最小化することで達成できます。

## 最終的な損失関数
VAEにおける目標は、変分下限$L(X,z)$の最大化です。そして、これを最大化するには、KLダイバージェンスを小さくして、$E_{q_{\phi}(z\mid X)}[\log p_{\theta}(X\mid z)]$を大きくする必要があります。従って、変分下限にマイナスをかけたものを損失関数とするのが良いことがわかります。KLダイバージェンスは、計算可能であるのでその値を用い、$E_{q_{\phi}(z\mid X)}[\log p_{\theta}(X\mid z)]$の評価には、BCELossを用いることにすると損失関数は次のように表されます。

$$Loss\_func=-(-D_{KL})+BCELoss$$

二つの正規分布$p(x)\sim N(\mu,\sigma^2),q(x)\sim N(0,1)$の間のKLダイバージェンスは、次のように表されます。[こちら](https://www.hello-statisticians.com/explain-terms-cat/kl_divergence1.html)を大変参考にしました。

$$D_{KL}(p||q)=\frac{1}{2}\left[-\log\sigma^2+\sigma^2-1+\mu^2\right]$$

VAEで用いるの多変量の正規分布であるためKLダイバージェンスの表現はこれよりも複雑になります。詳しく知りたい方は、以下の記事をご覧ください。丁寧に証明がなされています。

- [正規分布間のKLダイバージェンスの導出 - 唯物是真 @Scaled\_Wurm](https://sucrose.hatenablog.com/entry/2013/07/20/190146)
## Reparameterization Trick
下の図は、Kingma & Welling, NIPS workshop 2015から引用しました。非常にわかりやすいです。

![](https://storage.googleapis.com/zenn-user-upload/667c92eaab54-20230206.png)

下の画像は、[こちら](https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/structured-encoders-decoders.pdf)からお借りしました。こちらも非常に理解に役立ちました。

![](https://storage.googleapis.com/zenn-user-upload/902e76691a57-20230206.png)


VAEの構造では、逆伝播の途中に$z\sim N(\mu(X),\sigma(X))$という確率分布が入ります。確率が上の図を見てもわかるように確率が絡むと逆伝播の計算が確定できません。従って、乱数$\epsilon\sim N(0,1)$を用いて値を確定させます。従って、潜在変数$z$は下のように計算されます。

$$z=\mu(X)+\epsilon\cdot\sigma(X)$$

# 終わりに
今回の勉強を通して初めて、対数尤度や変分下限、KLダイバージェンスという概念に触れたため理解が浅い部分や言葉が足りない部分があると思います。疑問に感じた場合は、この記事を信じすぎず他の記事と見比べてください。

# 参考記事一覧
- [PyTorchでCVAEを実装してみた - Qiita](https://qiita.com/isuya/items/a856971647ba2390f5fe)
- [Variational Autoencoder徹底解説 - Qiita](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24#1-variational-autoencoder%E3%81%A8%E3%81%AF)
- [最尤法によるパラメータ推定の基礎を理解する(二項分布と正規分布のパラメータの最尤推定量の導出) - Qiita](https://qiita.com/g-k/items/4b2d557c24558074f07f)
- [イェンセン(Jensen)の不等式の直感的理解 - Qiita](https://qiita.com/kenmatsu4/items/26d098a4048f84bf85fb)
- [Jensen's inequality (イェンセンの不等式)のシンプルな証明](https://mcm-www.jwu.ac.jp/~konno/pdf/statg-1-8r.pdf)
- [【基礎から理解しよう】KLダイバージェンスの意味 - DXコンサルの日進月歩奮闘記](https://dx-consultant-fast-evolving.com/the-meaning-of-kl-divergence/)
- [モデル選択：周辺尤度最大化 | Pyroで実践するベイズ機械学習](https://pyro-book.data-hacker.net/docs/model_selection_01/)
- [機械学習のための確率基礎とベイズの定理 - HELLO CYBERNETICS](https://www.hellocybernetics.tech/entry/2016/10/05/163856#確率の乗法定理)
- [変分法をごまかさずに変分ベイズの説明をする - StatModeling Memorandum](https://statmodeling.hatenablog.com/entry/variational-bayesian-inference-1)
- [イェンセンの不等式とKLダイバージェンスの非負性｜Python実装で理解する変分推論(VariationalInference) #2 - Liberal Art’s diary](https://lib-arts.hatenablog.com/entry/VariationalInference2)
- [上に凸，下に凸な関数と二階微分 | 高校数学の美しい物語](https://manabitimes.jp/math/927)
- [1.1.8-10：カルバック・ライブラー・ダイバージェンスとイェンゼンの不等式【『トピックモデル』の勉強ノート】 - からっぽのしょこ](https://www.anarchive-beta.com/entry/2019/05/03/005902#1110-イェンゼンの不等式)
- [カルバックライブラーダイバージェンスとその周辺](https://zenn.dev/akio_tomiya/articles/29a3bb2d2c23f3)
- [正規分布間のKLダイバージェンスの導出 - 唯物是真 @Scaled\_Wurm](https://sucrose.hatenablog.com/entry/2013/07/20/190146)
- [正規分布間のKLダイバージェンス(KL-divergence)の値をグラフ化して把握する - あつまれ統計の森](https://www.hello-statisticians.com/explain-terms-cat/kl_divergence1.html)
- [The Reparameterization Trick](https://gregorygundersen.com/blog/2018/04/29/reparameterization/)
- [The Reparameterisation Trick|Variational Inference,Machine Learning and AI Academy](https://youtu.be/ptlKJ5WRdcs)
- [Deep Learning 23: (5) Variational AutoEncoder : Optimization and Reparametrization Trick, Ahlad Kumar](https://www.youtube.com/watch?v=7MVJ7FgrsYc)
- [The (Local) Reparameterization Trick, ](https://www.cs.toronto.edu/~duvenaud/courses/csc2541/slides/structured-encoders-decoders.pdf)
